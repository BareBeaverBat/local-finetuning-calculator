{
    "name": "gemma_7b",
    "num_embed_params": 786825216,
    "num_non_embed_params": 7751248896,
    "__comment_num_non_embed_params": "todo use num_non_embed_params to sanity-check the computation of that from # layers, d_model, feed-forward hidden dim, num query heads, head size, vocab size",
    "num_layers": 28,
    "model_dim": 3072,
    "feed_forward_hidden_dim": 49152,
    "__comment_on_feed_forward_hidden_dim": "this is the total width to which the representations get expanded in the hidden layer of the feed-forward block, but b/c this has gated MLP blocks the up projection and the gate projection actually each have a width that's 1/2 of this number",
    "num_query_heads": 16,
    "num_kv_heads": 16,
    "__comment_num_kv_heads": "keeping this option as numeric so this format can describe multi-query and multi-head attention models as well as any grouped-query attention model without an enum option for switchng between those",
    "head_size": 256,
    "vocab_size": 256128,
    "embed_module_name": "embed_tokens",
    "attn_module_names": ["q_proj", "k_proj", "v_proj", "o_proj"],
    "mlp_module_names": ["gate_proj", "up_proj", "down_proj"],
    "is_mlp_gated": true,
    "__comment_divider1": "the above are for principled considerations, the below for hacks to get around pytorch's/bitsandbytes' frustratingly coarse-grained and inexplicable memory allocation behavior",
    "initial_massive_params_chunk_size": 1572864000,
    "__comment_initial_massive_params_chunk": "this is the size of the big params chunk which is allocated at the start of the base-model-weights-4bit-quantization process and then deallocated immediately after the quantization process is done _and after_ the persistent massive params chunk is _allocated_",
    "total_size_of_frozen_weight_small_tensors": 3897891840,
    "__comment_total_size_of_frozen_weight_small_tensors": "this is the total size of the frozen weight tensors that are built up during the base-model-weights-4bit-quantization process and then persist throughout the training run",
    "persistent_massive_params_chunk_size": 3145728000,
    "__comment_persistent_massive_params_chunk": "this is the size of the big params chunk which is allocated at the end of the base-model-weights-4bit-quantization process is done and then persists through the rest of the training process"
}