{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:55:17.231434Z",
     "start_time": "2024-04-17T16:55:15.325119Z"
    }
   },
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, Dataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "50c8dfb9ed9d1242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:55:26.470849Z",
     "start_time": "2024-04-17T16:55:17.233517Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d8958b7b6bb1a452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:55:26.477332Z",
     "start_time": "2024-04-17T16:55:26.472755Z"
    }
   },
   "source": [
    "#notebook_login()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "996b8e751778e120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:15:29.852490Z",
     "start_time": "2024-04-17T16:55:26.480121Z"
    }
   },
   "source": [
    "eli5: Dataset = load_dataset(\"eli5_category\", split=\"train[:5000]\", trust_remote_code=True)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "12adddb58b5f39e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:15:29.888194Z",
     "start_time": "2024-04-17T17:15:29.856600Z"
    }
   },
   "source": [
    "eli5_train_test = eli5.train_test_split(test_size=0.2)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "382af01843cb8b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:15:29.907235Z",
     "start_time": "2024-04-17T17:15:29.890836Z"
    }
   },
   "source": "eli5_train_test[\"train\"][0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '7ggicc',\n",
       " 'title': 'How can a coat check say they are not responsible for damage or loss to my coat?',\n",
       " 'selftext': '',\n",
       " 'category': 'Other',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['dqiwen2'],\n",
       "  'text': [\"Because it doesn't really mean anything legally. Statements like this, signs in parking lots, and written liability waivers are largely for show. Were you to ignore the waiver, and sue for the cost of your lost jacket, the judge would not stop you because of their claim. They could still absolutely be liable for replacing the lost jacket. So why even bother having waivers in the first place? Because it makes customers think that they have no legal recourse, so they don't push the issue after an item is stollen or someone gets hurt. It also shows the potential judge that the customer was warned of the potential danger before participating, which can be a helpful defense in some personal injury cases.\"],\n",
       "  'score': [12],\n",
       "  'text_urls': [[]]},\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b6be1ca8c73ff384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:15:29.926270Z",
     "start_time": "2024-04-17T17:15:29.910409Z"
    }
   },
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,#todo explore whether it actually increases overall memory usage to change this to float32\n",
    "    bnb_4bit_use_double_quant=True# reminder that this makes computing the total memory used by the frozen weights even more complicated, something about reducing the size of the quantization constants that are used for remembering how to dequantize a given block of quantized weights? by the equivalent of 0.4 bits per parameter, per docs\n",
    "    \n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a9e5c90629ed8151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:22.671747Z",
     "start_time": "2024-04-17T17:15:29.928813Z"
    }
   },
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4850ffeeb524d778e8151af37906ef0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3eb0992414f9a31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.471557Z",
     "start_time": "2024-04-17T17:16:22.683125Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "31d4b9c83121bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.478679Z",
     "start_time": "2024-04-17T17:16:24.474260Z"
    }
   },
   "source": [
    "# tokenizer.padding_side = \"right\" #todo based on runtime warning while building SFTTrainer, can try this if problems occur, but by default I think I should respect the default for the Gemma Tokenizer"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "6cf6bd082de65da1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.495819Z",
     "start_time": "2024-04-17T17:16:24.481645Z"
    }
   },
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", # just lora'ing attention heads for now, to mimic original LoRA paper\n",
    "                    #\"gate_proj\", \"up_proj\", \"down_proj\" todo what is gate_proj? I think up_proj is first weights matrix of MLP block (fan out) and down_proj is second weights matrix of MLP block (fan in), but no idea what gate_proj is\n",
    "                    ],# reminder, can use \"all-linear\" (not inside list) for the expansive case https://huggingface.co/docs/peft/developer_guides/lora#qlora-style-training\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_rslora=True\n",
    "    #todo investigate whether it's worth trying Dora, iirc that was said to be especially helpful when lora rank is low\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "fc13a783360c7582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.519777Z",
     "start_time": "2024-04-17T17:16:24.500466Z"
    }
   },
   "source": [
    "seq_len = 128"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "24d60bf2a96170d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.535987Z",
     "start_time": "2024-04-17T17:16:24.524876Z"
    }
   },
   "source": [
    "# tokenizer"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "9c43405600c0b7aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.618674Z",
     "start_time": "2024-04-17T17:16:24.551824Z"
    }
   },
   "source": [
    "eli5_train_test = eli5_train_test.flatten()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "daa6fb5711441362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.626743Z",
     "start_time": "2024-04-17T17:16:24.621652Z"
    }
   },
   "source": [
    "# eli5_train_test[\"train\"][0]"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "c11c55165ed71a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:24.641262Z",
     "start_time": "2024-04-17T17:16:24.629082Z"
    }
   },
   "source": [
    "def fix_data(record):\n",
    "    '''\n",
    "    make dataset usable by TRL (i.e. its classes have a dataset_text_field param, and that column must be string-type, \n",
    "    not list<string> type)\n",
    "    :param record: record where the text column is actually a length-1 list column\n",
    "    :return: record where the text column is straightforwardly a text-type column\n",
    "    '''\n",
    "    record[\"answers.text\"] = record[\"answers.text\"][0]\n",
    "    return record"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "6774fe8dc74c1975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:25.947895Z",
     "start_time": "2024-04-17T17:16:24.643165Z"
    }
   },
   "source": [
    "eli5_train_test = eli5_train_test.map(fix_data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0460fa2f9c24f0384cfd45e17903931"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d425fef889b4404ba4f77cba037be9d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a573582827fb65bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:25.955847Z",
     "start_time": "2024-04-17T17:16:25.950358Z"
    }
   },
   "source": [
    "fixed_len_train_dset =  ConstantLengthDataset(tokenizer, eli5_train_test[\"train\"], \"answers.text\", seq_length=seq_len)\n",
    "fixed_len_eval_dset =  ConstantLengthDataset(tokenizer, eli5_train_test[\"test\"], \"answers.text\", seq_length=seq_len)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "8cba9279ee911528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:25.970770Z",
     "start_time": "2024-04-17T17:16:25.959608Z"
    }
   },
   "source": [
    "# fixed_len_train_dset.__iter__().__next__()"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "bb91189afd88e43a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:27.867197Z",
     "start_time": "2024-04-17T17:16:25.973571Z"
    }
   },
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=fixed_len_train_dset,\n",
    "    eval_dataset=fixed_len_eval_dset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        # gradient_accumulation_steps=4,#todo don't want to touch this until I understand it\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_32bit\"#can try paged_adamw_8bit in absolute worst case\n",
    "    ),\n",
    "    packing=True,\n",
    "    # dataset_text_field=\"answers.text\",\n",
    "    peft_config=lora_config\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scostesal/.virtualenvs/local-finetuning-calculator/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/scostesal/.virtualenvs/local-finetuning-calculator/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/scostesal/.virtualenvs/local-finetuning-calculator/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/scostesal/.virtualenvs/local-finetuning-calculator/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "15b1858ee0c5a093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:28.057411Z",
     "start_time": "2024-04-17T17:16:27.870640Z"
    }
   },
   "source": [
    "#todo set up pytorch gpu mem tracking\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "\n",
    "#investigate whether WSL 2 could be used to get around the \"no windows support for gpu mem visualization\" issue\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "87c463b4f651c2c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:16:28.087988Z",
     "start_time": "2024-04-17T17:16:28.060351Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "9bbb8eba92c1f2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:13.397845Z",
     "start_time": "2024-04-17T17:16:28.090671Z"
    }
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.856100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=5.436914730072021, metrics={'train_runtime': 45.1367, 'train_samples_per_second': 0.222, 'train_steps_per_second': 0.222, 'total_flos': 15227950202880.0, 'train_loss': 5.436914730072021, 'epoch': 0.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:13.405612Z",
     "start_time": "2024-04-17T17:17:13.400297Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "id": "91c5f6fc8a1cfa97",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:13.421219Z",
     "start_time": "2024-04-17T17:17:13.408670Z"
    }
   },
   "cell_type": "code",
   "source": "os.getcwd()",
   "id": "93caa19a6f6fe0ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/ssili/PycharmProjects/local-finetuning-calculator'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "894bb28458e3d280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:19.432374Z",
     "start_time": "2024-04-17T17:17:13.423991Z"
    }
   },
   "source": "torch.cuda.memory._dump_snapshot(\"gemma2b_2nd_mem_snapshot.pickle\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "8cba29dfdce13b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:19.439856Z",
     "start_time": "2024-04-17T17:17:19.435065Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "2e22d0047400941d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:19.448015Z",
     "start_time": "2024-04-17T17:17:19.443688Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "bc3774ab88ab1128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:19.455217Z",
     "start_time": "2024-04-17T17:17:19.450818Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "f3cd155151026d7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:17:19.463158Z",
     "start_time": "2024-04-17T17:17:19.457563Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
